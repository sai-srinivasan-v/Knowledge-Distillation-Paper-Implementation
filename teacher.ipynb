{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision numpy \n",
    "# !pip install torchsummary torchmetrics  \n",
    "# !pip install torchmetrics torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcs\n",
    "import torch\n",
    "import torchvision.transforms as transform\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import SGD,Adam\n",
    "import torch.functional as F\n",
    "from torch.utils.data import random_split,DataLoader\n",
    "import os\n",
    "import networks\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = funcs.get_GPU()\n",
    "device\n",
    "# Paths\n",
    "PATH = '/Users/sai/Desktop/Paper'\n",
    "checkpoints_path = 'checkpoints_teacher/'\n",
    "if not os.path.exists(checkpoints_path):\n",
    "    os.mkdir(checkpoints_path)\n",
    "IMAGE_SHAPE = (28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transform.Compose([transform.ToTensor(),\n",
    "                                     transform.Normalize((0.5,0.5),(0.5,0.5))]\n",
    ")\n",
    "\n",
    "test_transform = transform.Compose([transform.ToTensor(),\n",
    "                                     transform.Normalize((0.5,0.5),(0.5,0.5))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = tv.datasets.MNIST(root=PATH,\n",
    "                                      train=True,\n",
    "                                      transform=transform.ToTensor(),\n",
    "                                      download=True,target_transform=None)\n",
    "test_dataset = tv.datasets.MNIST(root=PATH,\n",
    "                                      train=False,\n",
    "                                      transform=transform.ToTensor(),\n",
    "                                      download=True,target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "funcs.visualise(train_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(train_val_dataset)\n",
    "val_size = int(total_size * 0.05)\n",
    "train_size =  total_size - val_size\n",
    "\n",
    "train_subset,val_subset = random_split(train_val_dataset,[train_size,val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(train_subset,batch_size=128,shuffle=True,num_workers=(os.cpu_count()//2))\n",
    "val_loader = DataLoader(val_subset,batch_size=128,shuffle=False,num_workers=(os.cpu_count()//2))\n",
    "test_loader = DataLoader(test_dataset,batch_size=128,shuffle=False,num_workers=(os.cpu_count()//2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam = {'dropout_input': 0.0,\n",
    " 'dropout_hidden': 0.0,\n",
    " 'weight_decay': 1e-05,\n",
    " 'lr_decay': 0.95,\n",
    " 'momentum': 0.9,\n",
    " 'lr': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "           Dropout-2                  [-1, 784]               0\n",
      "            Linear-3                 [-1, 1200]         942,000\n",
      "              ReLU-4                 [-1, 1200]               0\n",
      "           Dropout-5                 [-1, 1200]               0\n",
      "            Linear-6                 [-1, 1200]       1,441,200\n",
      "              ReLU-7                 [-1, 1200]               0\n",
      "           Dropout-8                 [-1, 1200]               0\n",
      "            Linear-9                   [-1, 10]          12,010\n",
      "================================================================\n",
      "Total params: 2,395,210\n",
      "Trainable params: 2,395,210\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.07\n",
      "Params size (MB): 9.14\n",
      "Estimated Total Size (MB): 9.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = networks.TeacherModel()\n",
    "summary(model,input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of TeacherModel(\n",
       "  (model): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "    (2): Linear(in_features=784, out_features=1200, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.0, inplace=False)\n",
       "    (5): Linear(in_features=1200, out_features=1200, bias=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.0, inplace=False)\n",
       "    (8): Linear(in_features=1200, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, training loss: 2.3060593605041504\n",
      "Epoch: 1, training loss: 2.275689125061035\n",
      "Epoch: 1, training loss: 2.203773021697998\n",
      "Epoch: 1, training loss: 2.1421871185302734\n",
      "Epoch: 1, training loss: 1.9737917184829712\n",
      "Epoch: 1, training loss: 1.8010097742080688\n",
      "Epoch: 1, training loss: 1.5554757118225098\n",
      "Epoch: 1, training loss: 1.23189115524292\n",
      "Epoch: 1, training loss: 0.9439579844474792\n",
      "Epoch: 1, training loss: 0.8206731081008911\n",
      "Epoch: 1, training loss: 0.7280183434486389\n",
      "Epoch: 1, training loss: 0.7542275786399841\n",
      "Epoch: 1, training loss: 0.4896538555622101\n",
      "Epoch: 1, training loss: 0.45811113715171814\n",
      "Epoch: 1, training loss: 0.5643460750579834\n",
      "Epoch: 1, training loss: 0.41173332929611206\n",
      "Epoch: 1, training loss: 0.4397623538970947\n",
      "Epoch: 1, training loss: 0.42940065264701843\n",
      "Epoch: 1, training loss: 0.40832632780075073\n",
      "Epoch: 1, training loss: 0.5714909434318542\n",
      "Epoch: 1, training loss: 0.3413066565990448\n",
      "Epoch: 1, training loss: 0.3847726285457611\n",
      "Epoch: 1, training loss: 0.20263655483722687\n",
      "Epoch: 1, training loss: 0.28405237197875977\n",
      "Epoch: 1, training loss: 0.3613038957118988\n",
      "Epoch: 1, training loss: 0.47300246357917786\n",
      "Epoch: 1, training loss: 0.3829611539840698\n",
      "Epoch: 1, training loss: 0.4094896614551544\n",
      "Epoch: 1, training loss: 0.2570224702358246\n",
      "Epoch: 1, training loss: 0.36185020208358765\n",
      "Epoch: 1, training loss: 0.3258507549762726\n",
      "Epoch: 1, training loss: 0.30726680159568787\n",
      "Epoch: 1, training loss: 0.26185357570648193\n",
      "Epoch: 1, training loss: 0.291559636592865\n",
      "Epoch: 1, training loss: 0.3197441101074219\n",
      "Epoch: 1, training loss: 0.27548399567604065\n",
      "Epoch: 1, training loss: 0.2862529158592224\n",
      "Epoch: 1, training loss: 0.38856321573257446\n",
      "Epoch: 1, training loss: 0.3259596824645996\n",
      "Epoch: 1, training loss: 0.22550983726978302\n",
      "Epoch: 1, training loss: 0.4371136426925659\n",
      "Epoch: 1, training loss: 0.2641094923019409\n",
      "Epoch: 1, training loss: 0.24516169726848602\n",
      "Epoch: 1, training loss: 0.33393391966819763\n",
      "Epoch: 1, training loss: 0.20368768274784088\n",
      "Epoch 1 Completed: Avg Loss: 0.6675, Accuracy: 82.24%\n",
      "Epoch: 1, AVG Loss: 0.0025132402429978054, accuracy per epoch: 91.13333333333333\n",
      "Epoch: 2, training loss: 0.22771847248077393\n",
      "Epoch: 2, training loss: 0.34031614661216736\n",
      "Epoch: 2, training loss: 0.3289259672164917\n",
      "Epoch: 2, training loss: 0.42474207282066345\n",
      "Epoch: 2, training loss: 0.2595904469490051\n",
      "Epoch: 2, training loss: 0.34481582045555115\n",
      "Epoch: 2, training loss: 0.26227232813835144\n",
      "Epoch: 2, training loss: 0.19996210932731628\n",
      "Epoch: 2, training loss: 0.2068835347890854\n",
      "Epoch: 2, training loss: 0.2271568924188614\n",
      "Epoch: 2, training loss: 0.24300161004066467\n",
      "Epoch: 2, training loss: 0.24505576491355896\n",
      "Epoch: 2, training loss: 0.20625388622283936\n",
      "Epoch: 2, training loss: 0.20766042172908783\n",
      "Epoch: 2, training loss: 0.2487386018037796\n",
      "Epoch: 2, training loss: 0.2938871681690216\n",
      "Epoch: 2, training loss: 0.23538163304328918\n",
      "Epoch: 2, training loss: 0.20756012201309204\n",
      "Epoch: 2, training loss: 0.273969441652298\n",
      "Epoch: 2, training loss: 0.26327040791511536\n",
      "Epoch: 2, training loss: 0.19272606074810028\n",
      "Epoch: 2, training loss: 0.2895219624042511\n",
      "Epoch: 2, training loss: 0.22953547537326813\n",
      "Epoch: 2, training loss: 0.24357613921165466\n",
      "Epoch: 2, training loss: 0.20785322785377502\n",
      "Epoch: 2, training loss: 0.16522181034088135\n",
      "Epoch: 2, training loss: 0.2509143054485321\n",
      "Epoch: 2, training loss: 0.2291506975889206\n",
      "Epoch: 2, training loss: 0.26533424854278564\n",
      "Epoch: 2, training loss: 0.27674296498298645\n",
      "Epoch: 2, training loss: 0.2643777132034302\n",
      "Epoch: 2, training loss: 0.35322317481040955\n",
      "Epoch: 2, training loss: 0.2694559395313263\n",
      "Epoch: 2, training loss: 0.3512437045574188\n",
      "Epoch: 2, training loss: 0.25358524918556213\n",
      "Epoch: 2, training loss: 0.24060477316379547\n",
      "Epoch: 2, training loss: 0.26273125410079956\n",
      "Epoch: 2, training loss: 0.24783575534820557\n",
      "Epoch: 2, training loss: 0.20012135803699493\n",
      "Epoch: 2, training loss: 0.19407467544078827\n",
      "Epoch: 2, training loss: 0.2837274372577667\n",
      "Epoch: 2, training loss: 0.1946316957473755\n",
      "Epoch: 2, training loss: 0.17453458905220032\n",
      "Epoch: 2, training loss: 0.1943461149930954\n",
      "Epoch: 2, training loss: 0.20915819704532623\n",
      "Epoch 2 Completed: Avg Loss: 0.2505, Accuracy: 92.77%\n",
      "Epoch: 2, AVG Loss: 0.0018375468651453653, accuracy per epoch: 93.26666666666667\n",
      "Epoch: 3, training loss: 0.17228837311267853\n",
      "Epoch: 3, training loss: 0.17767079174518585\n",
      "Epoch: 3, training loss: 0.13335761427879333\n",
      "Epoch: 3, training loss: 0.12166903913021088\n",
      "Epoch: 3, training loss: 0.24211235344409943\n",
      "Epoch: 3, training loss: 0.3096468448638916\n",
      "Epoch: 3, training loss: 0.1964264214038849\n",
      "Epoch: 3, training loss: 0.3802155554294586\n",
      "Epoch: 3, training loss: 0.16378220915794373\n",
      "Epoch: 3, training loss: 0.1498117595911026\n",
      "Epoch: 3, training loss: 0.20432402193546295\n",
      "Epoch: 3, training loss: 0.15252995491027832\n",
      "Epoch: 3, training loss: 0.14893756806850433\n",
      "Epoch: 3, training loss: 0.1928795427083969\n",
      "Epoch: 3, training loss: 0.23113645613193512\n",
      "Epoch: 3, training loss: 0.21757400035858154\n",
      "Epoch: 3, training loss: 0.11925069242715836\n",
      "Epoch: 3, training loss: 0.33383145928382874\n",
      "Epoch: 3, training loss: 0.1435326188802719\n",
      "Epoch: 3, training loss: 0.25926998257637024\n",
      "Epoch: 3, training loss: 0.29950517416000366\n",
      "Epoch: 3, training loss: 0.147357776761055\n",
      "Epoch: 3, training loss: 0.15215060114860535\n",
      "Epoch: 3, training loss: 0.1964307576417923\n",
      "Epoch: 3, training loss: 0.2969173192977905\n",
      "Epoch: 3, training loss: 0.3241751194000244\n",
      "Epoch: 3, training loss: 0.20514516532421112\n",
      "Epoch: 3, training loss: 0.24707402288913727\n",
      "Epoch: 3, training loss: 0.16236844658851624\n",
      "Epoch: 3, training loss: 0.17283178865909576\n",
      "Epoch: 3, training loss: 0.14308865368366241\n",
      "Epoch: 3, training loss: 0.20806701481342316\n",
      "Epoch: 3, training loss: 0.15620014071464539\n",
      "Epoch: 3, training loss: 0.2120393067598343\n",
      "Epoch: 3, training loss: 0.1447189748287201\n",
      "Epoch: 3, training loss: 0.1346307098865509\n",
      "Epoch: 3, training loss: 0.16484655439853668\n",
      "Epoch: 3, training loss: 0.1395326852798462\n",
      "Epoch: 3, training loss: 0.15129776298999786\n",
      "Epoch: 3, training loss: 0.1407478302717209\n",
      "Epoch: 3, training loss: 0.10313333570957184\n",
      "Epoch: 3, training loss: 0.11996720731258392\n",
      "Epoch: 3, training loss: 0.13279645144939423\n",
      "Epoch: 3, training loss: 0.1666773408651352\n",
      "Epoch: 3, training loss: 0.13879884779453278\n",
      "Epoch 3 Completed: Avg Loss: 0.1857, Accuracy: 94.64%\n",
      "Epoch: 3, AVG Loss: 0.0014711874847610793, accuracy per epoch: 94.23333333333333\n",
      "Epoch: 4, training loss: 0.08268225938081741\n",
      "Epoch: 4, training loss: 0.18077829480171204\n",
      "Epoch: 4, training loss: 0.12444373220205307\n",
      "Epoch: 4, training loss: 0.13534140586853027\n",
      "Epoch: 4, training loss: 0.12523414194583893\n",
      "Epoch: 4, training loss: 0.1333688348531723\n",
      "Epoch: 4, training loss: 0.1896100789308548\n",
      "Epoch: 4, training loss: 0.127883180975914\n",
      "Epoch: 4, training loss: 0.13442447781562805\n",
      "Epoch: 4, training loss: 0.08334184437990189\n",
      "Epoch: 4, training loss: 0.09956931322813034\n",
      "Epoch: 4, training loss: 0.08238313347101212\n",
      "Epoch: 4, training loss: 0.23185987770557404\n",
      "Epoch: 4, training loss: 0.16334940493106842\n",
      "Epoch: 4, training loss: 0.0980246439576149\n",
      "Epoch: 4, training loss: 0.22676800191402435\n",
      "Epoch: 4, training loss: 0.16738584637641907\n",
      "Epoch: 4, training loss: 0.08193496614694595\n",
      "Epoch: 4, training loss: 0.06863575428724289\n",
      "Epoch: 4, training loss: 0.06836564838886261\n",
      "Epoch: 4, training loss: 0.11930756270885468\n",
      "Epoch: 4, training loss: 0.18964126706123352\n",
      "Epoch: 4, training loss: 0.10778790712356567\n",
      "Epoch: 4, training loss: 0.1325988620519638\n",
      "Epoch: 4, training loss: 0.10889264941215515\n",
      "Epoch: 4, training loss: 0.10311222821474075\n",
      "Epoch: 4, training loss: 0.17241273820400238\n",
      "Epoch: 4, training loss: 0.21611997485160828\n",
      "Epoch: 4, training loss: 0.1606016904115677\n",
      "Epoch: 4, training loss: 0.07719781994819641\n",
      "Epoch: 4, training loss: 0.22198164463043213\n",
      "Epoch: 4, training loss: 0.05660977214574814\n",
      "Epoch: 4, training loss: 0.12130726873874664\n",
      "Epoch: 4, training loss: 0.1900397390127182\n",
      "Epoch: 4, training loss: 0.15509459376335144\n",
      "Epoch: 4, training loss: 0.08229945600032806\n",
      "Epoch: 4, training loss: 0.15973801910877228\n",
      "Epoch: 4, training loss: 0.11703605204820633\n",
      "Epoch: 4, training loss: 0.15528541803359985\n",
      "Epoch: 4, training loss: 0.1280663013458252\n",
      "Epoch: 4, training loss: 0.11031036078929901\n",
      "Epoch: 4, training loss: 0.13687969744205475\n",
      "Epoch: 4, training loss: 0.21940049529075623\n",
      "Epoch: 4, training loss: 0.12388291209936142\n",
      "Epoch: 4, training loss: 0.10863487422466278\n",
      "Epoch 4 Completed: Avg Loss: 0.1433, Accuracy: 95.91%\n",
      "Epoch: 4, AVG Loss: 0.001275301250318686, accuracy per epoch: 95.3\n",
      "Epoch: 5, training loss: 0.06807860732078552\n",
      "Epoch: 5, training loss: 0.11256005614995956\n",
      "Epoch: 5, training loss: 0.13647791743278503\n",
      "Epoch: 5, training loss: 0.05715547874569893\n",
      "Epoch: 5, training loss: 0.1175675168633461\n",
      "Epoch: 5, training loss: 0.1807221919298172\n",
      "Epoch: 5, training loss: 0.13698984682559967\n",
      "Epoch: 5, training loss: 0.12617839872837067\n",
      "Epoch: 5, training loss: 0.2054782658815384\n",
      "Epoch: 5, training loss: 0.09329825639724731\n",
      "Epoch: 5, training loss: 0.21151268482208252\n",
      "Epoch: 5, training loss: 0.10602441430091858\n",
      "Epoch: 5, training loss: 0.11621934175491333\n",
      "Epoch: 5, training loss: 0.11013642698526382\n",
      "Epoch: 5, training loss: 0.08809831738471985\n",
      "Epoch: 5, training loss: 0.2327275574207306\n",
      "Epoch: 5, training loss: 0.1132999137043953\n",
      "Epoch: 5, training loss: 0.12106893211603165\n",
      "Epoch: 5, training loss: 0.11816683411598206\n",
      "Epoch: 5, training loss: 0.07598765194416046\n",
      "Epoch: 5, training loss: 0.08885423094034195\n",
      "Epoch: 5, training loss: 0.06002165004611015\n",
      "Epoch: 5, training loss: 0.07125665992498398\n",
      "Epoch: 5, training loss: 0.13223494589328766\n",
      "Epoch: 5, training loss: 0.05819065123796463\n",
      "Epoch: 5, training loss: 0.1321370154619217\n",
      "Epoch: 5, training loss: 0.057103291153907776\n",
      "Epoch: 5, training loss: 0.13649654388427734\n",
      "Epoch: 5, training loss: 0.10352746397256851\n",
      "Epoch: 5, training loss: 0.1782136857509613\n",
      "Epoch: 5, training loss: 0.0713096410036087\n",
      "Epoch: 5, training loss: 0.17338725924491882\n",
      "Epoch: 5, training loss: 0.10031402856111526\n",
      "Epoch: 5, training loss: 0.12642818689346313\n",
      "Epoch: 5, training loss: 0.09251465648412704\n",
      "Epoch: 5, training loss: 0.04442288726568222\n",
      "Epoch: 5, training loss: 0.10166715830564499\n",
      "Epoch: 5, training loss: 0.12669378519058228\n",
      "Epoch: 5, training loss: 0.12028834968805313\n",
      "Epoch: 5, training loss: 0.1011740118265152\n",
      "Epoch: 5, training loss: 0.15088129043579102\n",
      "Epoch: 5, training loss: 0.07914716750383377\n",
      "Epoch: 5, training loss: 0.10225091129541397\n",
      "Epoch: 5, training loss: 0.08183685690164566\n",
      "Epoch: 5, training loss: 0.12168360501527786\n",
      "Epoch 5 Completed: Avg Loss: 0.1155, Accuracy: 96.69%\n",
      "Epoch: 5, AVG Loss: 0.0011019986222187677, accuracy per epoch: 95.7\n",
      "Epoch: 6, training loss: 0.06800289452075958\n",
      "Epoch: 6, training loss: 0.10456734895706177\n",
      "Epoch: 6, training loss: 0.08420496433973312\n",
      "Epoch: 6, training loss: 0.13134779036045074\n",
      "Epoch: 6, training loss: 0.07245910167694092\n",
      "Epoch: 6, training loss: 0.0419384203851223\n",
      "Epoch: 6, training loss: 0.09950988739728928\n",
      "Epoch: 6, training loss: 0.209318608045578\n",
      "Epoch: 6, training loss: 0.04694107547402382\n",
      "Epoch: 6, training loss: 0.10298839211463928\n",
      "Epoch: 6, training loss: 0.07083318382501602\n",
      "Epoch: 6, training loss: 0.06871102750301361\n",
      "Epoch: 6, training loss: 0.13884952664375305\n",
      "Epoch: 6, training loss: 0.11251706629991531\n",
      "Epoch: 6, training loss: 0.11691775918006897\n",
      "Epoch: 6, training loss: 0.174283966422081\n",
      "Epoch: 6, training loss: 0.08081125468015671\n",
      "Epoch: 6, training loss: 0.037114426493644714\n",
      "Epoch: 6, training loss: 0.050905708223581314\n",
      "Epoch: 6, training loss: 0.09709861129522324\n",
      "Epoch: 6, training loss: 0.07679460942745209\n",
      "Epoch: 6, training loss: 0.06314760446548462\n",
      "Epoch: 6, training loss: 0.10260088741779327\n",
      "Epoch: 6, training loss: 0.11223335564136505\n",
      "Epoch: 6, training loss: 0.03954735025763512\n",
      "Epoch: 6, training loss: 0.06499842554330826\n",
      "Epoch: 6, training loss: 0.043011609464883804\n",
      "Epoch: 6, training loss: 0.09812670946121216\n",
      "Epoch: 6, training loss: 0.1480000764131546\n",
      "Epoch: 6, training loss: 0.07978831231594086\n",
      "Epoch: 6, training loss: 0.06315986067056656\n",
      "Epoch: 6, training loss: 0.1603904515504837\n",
      "Epoch: 6, training loss: 0.086460180580616\n",
      "Epoch: 6, training loss: 0.09736470133066177\n",
      "Epoch: 6, training loss: 0.1022367998957634\n",
      "Epoch: 6, training loss: 0.11939068883657455\n",
      "Epoch: 6, training loss: 0.07247241586446762\n",
      "Epoch: 6, training loss: 0.12566636502742767\n",
      "Epoch: 6, training loss: 0.15872123837471008\n",
      "Epoch: 6, training loss: 0.06565145403146744\n",
      "Epoch: 6, training loss: 0.08146560192108154\n",
      "Epoch: 6, training loss: 0.08552640676498413\n",
      "Epoch: 6, training loss: 0.07347792387008667\n",
      "Epoch: 6, training loss: 0.09594620764255524\n",
      "Epoch: 6, training loss: 0.07867465168237686\n",
      "Epoch 6 Completed: Avg Loss: 0.0958, Accuracy: 97.27%\n",
      "Epoch: 6, AVG Loss: 0.0009414788211385409, accuracy per epoch: 96.23333333333333\n",
      "Epoch: 7, training loss: 0.07220757752656937\n",
      "Epoch: 7, training loss: 0.0781860277056694\n",
      "Epoch: 7, training loss: 0.05035264790058136\n",
      "Epoch: 7, training loss: 0.05905524268746376\n",
      "Epoch: 7, training loss: 0.1857219636440277\n",
      "Epoch: 7, training loss: 0.05023961141705513\n",
      "Epoch: 7, training loss: 0.13868436217308044\n",
      "Epoch: 7, training loss: 0.06707113236188889\n",
      "Epoch: 7, training loss: 0.04690307006239891\n",
      "Epoch: 7, training loss: 0.1766497790813446\n",
      "Epoch: 7, training loss: 0.10588767379522324\n",
      "Epoch: 7, training loss: 0.07369323074817657\n",
      "Epoch: 7, training loss: 0.13243195414543152\n",
      "Epoch: 7, training loss: 0.07281560450792313\n",
      "Epoch: 7, training loss: 0.08078761398792267\n",
      "Epoch: 7, training loss: 0.050834789872169495\n",
      "Epoch: 7, training loss: 0.03448738530278206\n",
      "Epoch: 7, training loss: 0.07717426121234894\n",
      "Epoch: 7, training loss: 0.22985702753067017\n",
      "Epoch: 7, training loss: 0.10559192299842834\n",
      "Epoch: 7, training loss: 0.10441958159208298\n",
      "Epoch: 7, training loss: 0.04426629841327667\n",
      "Epoch: 7, training loss: 0.07594111561775208\n",
      "Epoch: 7, training loss: 0.09610233455896378\n",
      "Epoch: 7, training loss: 0.03926672786474228\n",
      "Epoch: 7, training loss: 0.11325222998857498\n",
      "Epoch: 7, training loss: 0.0894254520535469\n",
      "Epoch: 7, training loss: 0.0685599148273468\n",
      "Epoch: 7, training loss: 0.03150066360831261\n",
      "Epoch: 7, training loss: 0.10549385100603104\n",
      "Epoch: 7, training loss: 0.07853086292743683\n",
      "Epoch: 7, training loss: 0.08529094606637955\n",
      "Epoch: 7, training loss: 0.037493765354156494\n",
      "Epoch: 7, training loss: 0.13617925345897675\n",
      "Epoch: 7, training loss: 0.053837601095438004\n",
      "Epoch: 7, training loss: 0.10610250383615494\n",
      "Epoch: 7, training loss: 0.1538967788219452\n",
      "Epoch: 7, training loss: 0.050203509628772736\n",
      "Epoch: 7, training loss: 0.031547509133815765\n",
      "Epoch: 7, training loss: 0.09903314709663391\n",
      "Epoch: 7, training loss: 0.035535503178834915\n",
      "Epoch: 7, training loss: 0.13295531272888184\n",
      "Epoch: 7, training loss: 0.07253823429346085\n",
      "Epoch: 7, training loss: 0.09696023166179657\n",
      "Epoch: 7, training loss: 0.038178008049726486\n",
      "Epoch 7 Completed: Avg Loss: 0.0808, Accuracy: 97.77%\n",
      "Epoch: 7, AVG Loss: 0.0009215758008261521, accuracy per epoch: 96.2\n",
      "Epoch: 8, training loss: 0.05077511444687843\n",
      "Epoch: 8, training loss: 0.07027681916952133\n",
      "Epoch: 8, training loss: 0.03737379238009453\n",
      "Epoch: 8, training loss: 0.08516357839107513\n",
      "Epoch: 8, training loss: 0.045564960688352585\n",
      "Epoch: 8, training loss: 0.06994074583053589\n",
      "Epoch: 8, training loss: 0.08427103608846664\n",
      "Epoch: 8, training loss: 0.06549516320228577\n",
      "Epoch: 8, training loss: 0.06383717060089111\n",
      "Epoch: 8, training loss: 0.09053236246109009\n",
      "Epoch: 8, training loss: 0.03404183313250542\n",
      "Epoch: 8, training loss: 0.06605570018291473\n",
      "Epoch: 8, training loss: 0.02594131790101528\n",
      "Epoch: 8, training loss: 0.05626584589481354\n",
      "Epoch: 8, training loss: 0.12413685023784637\n",
      "Epoch: 8, training loss: 0.10569385439157486\n",
      "Epoch: 8, training loss: 0.0379725806415081\n",
      "Epoch: 8, training loss: 0.07091808319091797\n",
      "Epoch: 8, training loss: 0.12742328643798828\n",
      "Epoch: 8, training loss: 0.05165417492389679\n",
      "Epoch: 8, training loss: 0.05250765383243561\n",
      "Epoch: 8, training loss: 0.052197787910699844\n",
      "Epoch: 8, training loss: 0.06377313286066055\n",
      "Epoch: 8, training loss: 0.04787931591272354\n",
      "Epoch: 8, training loss: 0.0949961468577385\n",
      "Epoch: 8, training loss: 0.05410805344581604\n",
      "Epoch: 8, training loss: 0.05927056819200516\n",
      "Epoch: 8, training loss: 0.05281379073858261\n",
      "Epoch: 8, training loss: 0.04375850781798363\n",
      "Epoch: 8, training loss: 0.07490351051092148\n",
      "Epoch: 8, training loss: 0.036100760102272034\n",
      "Epoch: 8, training loss: 0.053141385316848755\n",
      "Epoch: 8, training loss: 0.08074204623699188\n",
      "Epoch: 8, training loss: 0.04951853305101395\n",
      "Epoch: 8, training loss: 0.059850603342056274\n",
      "Epoch: 8, training loss: 0.03494228795170784\n",
      "Epoch: 8, training loss: 0.05898140370845795\n",
      "Epoch: 8, training loss: 0.08807485550642014\n",
      "Epoch: 8, training loss: 0.01921255886554718\n",
      "Epoch: 8, training loss: 0.11446521431207657\n",
      "Epoch: 8, training loss: 0.022042082622647285\n",
      "Epoch: 8, training loss: 0.12996533513069153\n",
      "Epoch: 8, training loss: 0.0687675029039383\n",
      "Epoch: 8, training loss: 0.042298149317502975\n",
      "Epoch: 8, training loss: 0.06587829440832138\n",
      "Epoch 8 Completed: Avg Loss: 0.0690, Accuracy: 98.05%\n",
      "Epoch: 8, AVG Loss: 0.0008118177503347397, accuracy per epoch: 96.6\n",
      "Epoch: 9, training loss: 0.02989458478987217\n",
      "Epoch: 9, training loss: 0.05394376441836357\n",
      "Epoch: 9, training loss: 0.09819531440734863\n",
      "Epoch: 9, training loss: 0.04183316230773926\n",
      "Epoch: 9, training loss: 0.0726088210940361\n",
      "Epoch: 9, training loss: 0.10122126340866089\n",
      "Epoch: 9, training loss: 0.03183547407388687\n",
      "Epoch: 9, training loss: 0.06239501014351845\n",
      "Epoch: 9, training loss: 0.0850265696644783\n",
      "Epoch: 9, training loss: 0.04106388986110687\n",
      "Epoch: 9, training loss: 0.02716290019452572\n",
      "Epoch: 9, training loss: 0.060332152992486954\n",
      "Epoch: 9, training loss: 0.04236967861652374\n",
      "Epoch: 9, training loss: 0.04310707002878189\n",
      "Epoch: 9, training loss: 0.0732889249920845\n",
      "Epoch: 9, training loss: 0.050423961132764816\n",
      "Epoch: 9, training loss: 0.04163677245378494\n",
      "Epoch: 9, training loss: 0.09444825351238251\n",
      "Epoch: 9, training loss: 0.044870633631944656\n",
      "Epoch: 9, training loss: 0.060818951576948166\n",
      "Epoch: 9, training loss: 0.0217171348631382\n",
      "Epoch: 9, training loss: 0.06078638136386871\n",
      "Epoch: 9, training loss: 0.10648234188556671\n",
      "Epoch: 9, training loss: 0.0995354875922203\n",
      "Epoch: 9, training loss: 0.06732746958732605\n",
      "Epoch: 9, training loss: 0.04210672155022621\n",
      "Epoch: 9, training loss: 0.17233221232891083\n",
      "Epoch: 9, training loss: 0.03851032629609108\n",
      "Epoch: 9, training loss: 0.019704293459653854\n",
      "Epoch: 9, training loss: 0.055894676595926285\n",
      "Epoch: 9, training loss: 0.04709640145301819\n",
      "Epoch: 9, training loss: 0.06722927838563919\n",
      "Epoch: 9, training loss: 0.22402323782444\n",
      "Epoch: 9, training loss: 0.06929081678390503\n",
      "Epoch: 9, training loss: 0.030457090586423874\n",
      "Epoch: 9, training loss: 0.07098165154457092\n",
      "Epoch: 9, training loss: 0.07950965315103531\n",
      "Epoch: 9, training loss: 0.07457932829856873\n",
      "Epoch: 9, training loss: 0.05331164598464966\n",
      "Epoch: 9, training loss: 0.10899215191602707\n",
      "Epoch: 9, training loss: 0.04989228770136833\n",
      "Epoch: 9, training loss: 0.043050602078437805\n",
      "Epoch: 9, training loss: 0.06817856431007385\n",
      "Epoch: 9, training loss: 0.05536121875047684\n",
      "Epoch: 9, training loss: 0.04403451830148697\n",
      "Epoch 9 Completed: Avg Loss: 0.0594, Accuracy: 98.29%\n",
      "Epoch: 9, AVG Loss: 0.0008001608848571777, accuracy per epoch: 96.83333333333334\n",
      "Epoch: 10, training loss: 0.08867921680212021\n",
      "Epoch: 10, training loss: 0.05345667898654938\n",
      "Epoch: 10, training loss: 0.03429696708917618\n",
      "Epoch: 10, training loss: 0.03824446722865105\n",
      "Epoch: 10, training loss: 0.11270759254693985\n",
      "Epoch: 10, training loss: 0.041198428720235825\n",
      "Epoch: 10, training loss: 0.03618945926427841\n",
      "Epoch: 10, training loss: 0.03880024701356888\n",
      "Epoch: 10, training loss: 0.05893552303314209\n",
      "Epoch: 10, training loss: 0.07311303168535233\n",
      "Epoch: 10, training loss: 0.05659939721226692\n",
      "Epoch: 10, training loss: 0.05262955650687218\n",
      "Epoch: 10, training loss: 0.03998636454343796\n",
      "Epoch: 10, training loss: 0.06597525626420975\n",
      "Epoch: 10, training loss: 0.021232880651950836\n",
      "Epoch: 10, training loss: 0.06277987360954285\n",
      "Epoch: 10, training loss: 0.06057705730199814\n",
      "Epoch: 10, training loss: 0.03322940319776535\n",
      "Epoch: 10, training loss: 0.09446324408054352\n",
      "Epoch: 10, training loss: 0.03707144781947136\n",
      "Epoch: 10, training loss: 0.018384840339422226\n",
      "Epoch: 10, training loss: 0.06017613410949707\n",
      "Epoch: 10, training loss: 0.053428854793310165\n",
      "Epoch: 10, training loss: 0.025501850992441177\n",
      "Epoch: 10, training loss: 0.09382621198892593\n",
      "Epoch: 10, training loss: 0.09703081101179123\n",
      "Epoch: 10, training loss: 0.0927746519446373\n",
      "Epoch: 10, training loss: 0.0338168703019619\n",
      "Epoch: 10, training loss: 0.014467664994299412\n",
      "Epoch: 10, training loss: 0.018373658880591393\n",
      "Epoch: 10, training loss: 0.04963325709104538\n",
      "Epoch: 10, training loss: 0.051177699118852615\n",
      "Epoch: 10, training loss: 0.06136166676878929\n",
      "Epoch: 10, training loss: 0.05067896097898483\n",
      "Epoch: 10, training loss: 0.024464301764965057\n",
      "Epoch: 10, training loss: 0.06295794248580933\n",
      "Epoch: 10, training loss: 0.02194632589817047\n",
      "Epoch: 10, training loss: 0.03560686483979225\n",
      "Epoch: 10, training loss: 0.048289116472005844\n",
      "Epoch: 10, training loss: 0.03539067879319191\n",
      "Epoch: 10, training loss: 0.01915939711034298\n",
      "Epoch: 10, training loss: 0.04224655404686928\n",
      "Epoch: 10, training loss: 0.03073933720588684\n",
      "Epoch: 10, training loss: 0.007516311015933752\n",
      "Epoch: 10, training loss: 0.039171524345874786\n",
      "Epoch 10 Completed: Avg Loss: 0.0510, Accuracy: 98.60%\n",
      "Epoch: 10, AVG Loss: 0.0007925871002177398, accuracy per epoch: 96.96666666666667\n",
      "Epoch: 11, training loss: 0.07899288088083267\n",
      "Epoch: 11, training loss: 0.029202844947576523\n",
      "Epoch: 11, training loss: 0.031089426949620247\n",
      "Epoch: 11, training loss: 0.031068123877048492\n",
      "Epoch: 11, training loss: 0.04561607912182808\n",
      "Epoch: 11, training loss: 0.03420815244317055\n",
      "Epoch: 11, training loss: 0.026463573798537254\n",
      "Epoch: 11, training loss: 0.028527196496725082\n",
      "Epoch: 11, training loss: 0.06524497270584106\n",
      "Epoch: 11, training loss: 0.019683528691530228\n",
      "Epoch: 11, training loss: 0.03445877507328987\n",
      "Epoch: 11, training loss: 0.08600328862667084\n",
      "Epoch: 11, training loss: 0.04213516041636467\n",
      "Epoch: 11, training loss: 0.05607147142291069\n",
      "Epoch: 11, training loss: 0.04907788708806038\n",
      "Epoch: 11, training loss: 0.026530588045716286\n",
      "Epoch: 11, training loss: 0.14341479539871216\n",
      "Epoch: 11, training loss: 0.041053976863622665\n",
      "Epoch: 11, training loss: 0.03815547749400139\n",
      "Epoch: 11, training loss: 0.04015028476715088\n",
      "Epoch: 11, training loss: 0.02000332437455654\n",
      "Epoch: 11, training loss: 0.022829895839095116\n",
      "Epoch: 11, training loss: 0.03663335740566254\n",
      "Epoch: 11, training loss: 0.06515732407569885\n",
      "Epoch: 11, training loss: 0.034744251519441605\n",
      "Epoch: 11, training loss: 0.013612061738967896\n",
      "Epoch: 11, training loss: 0.0373595654964447\n",
      "Epoch: 11, training loss: 0.08284913003444672\n",
      "Epoch: 11, training loss: 0.025506479665637016\n",
      "Epoch: 11, training loss: 0.02766932174563408\n",
      "Epoch: 11, training loss: 0.048178572207689285\n",
      "Epoch: 11, training loss: 0.021567627787590027\n",
      "Epoch: 11, training loss: 0.023699238896369934\n",
      "Epoch: 11, training loss: 0.026402996852993965\n",
      "Epoch: 11, training loss: 0.059799496084451675\n",
      "Epoch: 11, training loss: 0.038995228707790375\n",
      "Epoch: 11, training loss: 0.057603415101766586\n",
      "Epoch: 11, training loss: 0.033202268183231354\n",
      "Epoch: 11, training loss: 0.05135753005743027\n",
      "Epoch: 11, training loss: 0.023263582959771156\n",
      "Epoch: 11, training loss: 0.026643741875886917\n",
      "Epoch: 11, training loss: 0.03770504146814346\n",
      "Epoch: 11, training loss: 0.07167145609855652\n",
      "Epoch: 11, training loss: 0.09285404533147812\n",
      "Epoch: 11, training loss: 0.08696465194225311\n",
      "Epoch 11 Completed: Avg Loss: 0.0444, Accuracy: 98.76%\n",
      "Epoch: 11, AVG Loss: 0.0007270969959596793, accuracy per epoch: 97.5\n",
      "Epoch: 12, training loss: 0.026195386424660683\n",
      "Epoch: 12, training loss: 0.05998373404145241\n",
      "Epoch: 12, training loss: 0.042734742164611816\n",
      "Epoch: 12, training loss: 0.03632546588778496\n",
      "Epoch: 12, training loss: 0.04212469980120659\n",
      "Epoch: 12, training loss: 0.03952495753765106\n",
      "Epoch: 12, training loss: 0.029274452477693558\n",
      "Epoch: 12, training loss: 0.04424317181110382\n",
      "Epoch: 12, training loss: 0.0106828473508358\n",
      "Epoch: 12, training loss: 0.12273658066987991\n",
      "Epoch: 12, training loss: 0.030273979529738426\n",
      "Epoch: 12, training loss: 0.025244979187846184\n",
      "Epoch: 12, training loss: 0.039260607212781906\n",
      "Epoch: 12, training loss: 0.025424864143133163\n",
      "Epoch: 12, training loss: 0.022431809455156326\n",
      "Epoch: 12, training loss: 0.012906408868730068\n",
      "Epoch: 12, training loss: 0.0499626100063324\n",
      "Epoch: 12, training loss: 0.12337283045053482\n",
      "Epoch: 12, training loss: 0.03846912831068039\n",
      "Epoch: 12, training loss: 0.042592860758304596\n",
      "Epoch: 12, training loss: 0.05986301973462105\n",
      "Epoch: 12, training loss: 0.03050459362566471\n",
      "Epoch: 12, training loss: 0.0730036199092865\n",
      "Epoch: 12, training loss: 0.03739560768008232\n",
      "Epoch: 12, training loss: 0.04080962762236595\n",
      "Epoch: 12, training loss: 0.04340294748544693\n",
      "Epoch: 12, training loss: 0.09410431236028671\n",
      "Epoch: 12, training loss: 0.022911598905920982\n",
      "Epoch: 12, training loss: 0.03736838698387146\n",
      "Epoch: 12, training loss: 0.04905717819929123\n",
      "Epoch: 12, training loss: 0.06678324937820435\n",
      "Epoch: 12, training loss: 0.06513814628124237\n",
      "Epoch: 12, training loss: 0.02968958206474781\n",
      "Epoch: 12, training loss: 0.03157190605998039\n",
      "Epoch: 12, training loss: 0.07082460075616837\n",
      "Epoch: 12, training loss: 0.02137296088039875\n",
      "Epoch: 12, training loss: 0.02373308315873146\n",
      "Epoch: 12, training loss: 0.034013163298368454\n",
      "Epoch: 12, training loss: 0.0384100042283535\n",
      "Epoch: 12, training loss: 0.021054962649941444\n",
      "Epoch: 12, training loss: 0.021260615438222885\n",
      "Epoch: 12, training loss: 0.02633633278310299\n",
      "Epoch: 12, training loss: 0.03082885779440403\n",
      "Epoch: 12, training loss: 0.02035735920071602\n",
      "Epoch: 12, training loss: 0.019526729360222816\n",
      "Epoch 12 Completed: Avg Loss: 0.0390, Accuracy: 98.93%\n",
      "Epoch: 12, AVG Loss: 0.000694708381469051, accuracy per epoch: 97.46666666666667\n",
      "Epoch: 13, training loss: 0.022855369374155998\n",
      "Epoch: 13, training loss: 0.037747107446193695\n",
      "Epoch: 13, training loss: 0.03548482060432434\n",
      "Epoch: 13, training loss: 0.03296465799212456\n",
      "Epoch: 13, training loss: 0.028616372495889664\n",
      "Epoch: 13, training loss: 0.027248751372098923\n",
      "Epoch: 13, training loss: 0.009262326173484325\n",
      "Epoch: 13, training loss: 0.02396342158317566\n",
      "Epoch: 13, training loss: 0.017772167921066284\n",
      "Epoch: 13, training loss: 0.00902364682406187\n",
      "Epoch: 13, training loss: 0.055365998297929764\n",
      "Epoch: 13, training loss: 0.01642727665603161\n",
      "Epoch: 13, training loss: 0.02913978323340416\n",
      "Epoch: 13, training loss: 0.04552135616540909\n",
      "Epoch: 13, training loss: 0.03244967386126518\n",
      "Epoch: 13, training loss: 0.04294263944029808\n",
      "Epoch: 13, training loss: 0.03087202087044716\n",
      "Epoch: 13, training loss: 0.04544437676668167\n",
      "Epoch: 13, training loss: 0.0373757965862751\n",
      "Epoch: 13, training loss: 0.02071172557771206\n",
      "Epoch: 13, training loss: 0.07913874834775925\n",
      "Epoch: 13, training loss: 0.010501707904040813\n",
      "Epoch: 13, training loss: 0.033257413655519485\n",
      "Epoch: 13, training loss: 0.04731828719377518\n",
      "Epoch: 13, training loss: 0.036301203072071075\n",
      "Epoch: 13, training loss: 0.03447740525007248\n",
      "Epoch: 13, training loss: 0.015517479740083218\n",
      "Epoch: 13, training loss: 0.023425864055752754\n",
      "Epoch: 13, training loss: 0.036567769944667816\n",
      "Epoch: 13, training loss: 0.036640558391809464\n",
      "Epoch: 13, training loss: 0.022372426465153694\n",
      "Epoch: 13, training loss: 0.015636909753084183\n",
      "Epoch: 13, training loss: 0.06503759324550629\n",
      "Epoch: 13, training loss: 0.053629834204912186\n",
      "Epoch: 13, training loss: 0.038698483258485794\n",
      "Epoch: 13, training loss: 0.014898945577442646\n",
      "Epoch: 13, training loss: 0.015304935164749622\n",
      "Epoch: 13, training loss: 0.024782689288258553\n",
      "Epoch: 13, training loss: 0.021261604502797127\n",
      "Epoch: 13, training loss: 0.04504132643342018\n",
      "Epoch: 13, training loss: 0.035053323954343796\n",
      "Epoch: 13, training loss: 0.01672421395778656\n",
      "Epoch: 13, training loss: 0.04851581156253815\n",
      "Epoch: 13, training loss: 0.02875678800046444\n",
      "Epoch: 13, training loss: 0.02072918228805065\n",
      "Epoch 13 Completed: Avg Loss: 0.0336, Accuracy: 99.14%\n",
      "Epoch: 13, AVG Loss: 0.0006964019667357206, accuracy per epoch: 97.43333333333334\n",
      "Epoch: 14, training loss: 0.03246181458234787\n",
      "Epoch: 14, training loss: 0.011808452196419239\n",
      "Epoch: 14, training loss: 0.06892653554677963\n",
      "Epoch: 14, training loss: 0.022295840084552765\n",
      "Epoch: 14, training loss: 0.023609699681401253\n",
      "Epoch: 14, training loss: 0.006654080934822559\n",
      "Epoch: 14, training loss: 0.03602595627307892\n",
      "Epoch: 14, training loss: 0.009406046941876411\n",
      "Epoch: 14, training loss: 0.012400556355714798\n",
      "Epoch: 14, training loss: 0.030067719519138336\n",
      "Epoch: 14, training loss: 0.044883519411087036\n",
      "Epoch: 14, training loss: 0.040826451033353806\n",
      "Epoch: 14, training loss: 0.013914632610976696\n",
      "Epoch: 14, training loss: 0.018962044268846512\n",
      "Epoch: 14, training loss: 0.05958881601691246\n",
      "Epoch: 14, training loss: 0.03292768448591232\n",
      "Epoch: 14, training loss: 0.029878221452236176\n",
      "Epoch: 14, training loss: 0.03488871827721596\n",
      "Epoch: 14, training loss: 0.034401148557662964\n",
      "Epoch: 14, training loss: 0.035978272557258606\n",
      "Epoch: 14, training loss: 0.010046780109405518\n",
      "Epoch: 14, training loss: 0.04849162697792053\n",
      "Epoch: 14, training loss: 0.02001618780195713\n",
      "Epoch: 14, training loss: 0.04452646151185036\n",
      "Epoch: 14, training loss: 0.015305194072425365\n",
      "Epoch: 14, training loss: 0.07156049460172653\n",
      "Epoch: 14, training loss: 0.009869314730167389\n",
      "Epoch: 14, training loss: 0.02642204612493515\n",
      "Epoch: 14, training loss: 0.0219708364456892\n",
      "Epoch: 14, training loss: 0.0111029502004385\n",
      "Epoch: 14, training loss: 0.010741796344518661\n",
      "Epoch: 14, training loss: 0.04716993868350983\n",
      "Epoch: 14, training loss: 0.03830980509519577\n",
      "Epoch: 14, training loss: 0.03626072779297829\n",
      "Epoch: 14, training loss: 0.027754895389080048\n",
      "Epoch: 14, training loss: 0.048398587852716446\n",
      "Epoch: 14, training loss: 0.027529798448085785\n",
      "Epoch: 14, training loss: 0.013946372084319592\n",
      "Epoch: 14, training loss: 0.022958165034651756\n",
      "Epoch: 14, training loss: 0.018053658306598663\n",
      "Epoch: 14, training loss: 0.01929434947669506\n",
      "Epoch: 14, training loss: 0.08045484125614166\n",
      "Epoch: 14, training loss: 0.0496489480137825\n",
      "Epoch: 14, training loss: 0.032703060656785965\n",
      "Epoch: 14, training loss: 0.01385571900755167\n",
      "Epoch 14 Completed: Avg Loss: 0.0297, Accuracy: 99.27%\n",
      "Epoch: 14, AVG Loss: 0.0006950650655974945, accuracy per epoch: 97.53333333333333\n",
      "Epoch: 15, training loss: 0.008963499218225479\n",
      "Epoch: 15, training loss: 0.014462582767009735\n",
      "Epoch: 15, training loss: 0.022085128352046013\n",
      "Epoch: 15, training loss: 0.008466892875730991\n",
      "Epoch: 15, training loss: 0.01875777170062065\n",
      "Epoch: 15, training loss: 0.03302513062953949\n",
      "Epoch: 15, training loss: 0.03073125146329403\n",
      "Epoch: 15, training loss: 0.050737522542476654\n",
      "Epoch: 15, training loss: 0.04606682434678078\n",
      "Epoch: 15, training loss: 0.030508577823638916\n",
      "Epoch: 15, training loss: 0.03893989324569702\n",
      "Epoch: 15, training loss: 0.03754663094878197\n",
      "Epoch: 15, training loss: 0.09106142818927765\n",
      "Epoch: 15, training loss: 0.01757403276860714\n",
      "Epoch: 15, training loss: 0.01962135173380375\n",
      "Epoch: 15, training loss: 0.020316720008850098\n",
      "Epoch: 15, training loss: 0.017409391701221466\n",
      "Epoch: 15, training loss: 0.016088880598545074\n",
      "Epoch: 15, training loss: 0.024247558787465096\n",
      "Epoch: 15, training loss: 0.014613443985581398\n",
      "Epoch: 15, training loss: 0.011502706445753574\n",
      "Epoch: 15, training loss: 0.024144314229488373\n",
      "Epoch: 15, training loss: 0.015055101364850998\n",
      "Epoch: 15, training loss: 0.014218292199075222\n",
      "Epoch: 15, training loss: 0.020343001931905746\n",
      "Epoch: 15, training loss: 0.025154048576951027\n",
      "Epoch: 15, training loss: 0.04842517897486687\n",
      "Epoch: 15, training loss: 0.01906382106244564\n",
      "Epoch: 15, training loss: 0.021021734923124313\n",
      "Epoch: 15, training loss: 0.03190811350941658\n",
      "Epoch: 15, training loss: 0.026486970484256744\n",
      "Epoch: 15, training loss: 0.01550194900482893\n",
      "Epoch: 15, training loss: 0.025586849078536034\n",
      "Epoch: 15, training loss: 0.011887497268617153\n",
      "Epoch: 15, training loss: 0.022122832015156746\n",
      "Epoch: 15, training loss: 0.007004464510828257\n",
      "Epoch: 15, training loss: 0.02763625979423523\n",
      "Epoch: 15, training loss: 0.06356758624315262\n",
      "Epoch: 15, training loss: 0.045095447450876236\n",
      "Epoch: 15, training loss: 0.027608150616288185\n",
      "Epoch: 15, training loss: 0.023123718798160553\n",
      "Epoch: 15, training loss: 0.019825011491775513\n",
      "Epoch: 15, training loss: 0.02123476006090641\n",
      "Epoch: 15, training loss: 0.027970561757683754\n",
      "Epoch: 15, training loss: 0.006859298795461655\n",
      "Epoch 15 Completed: Avg Loss: 0.0263, Accuracy: 99.34%\n",
      "Epoch: 15, AVG Loss: 0.0006596253657092651, accuracy per epoch: 97.76666666666667\n",
      "Epoch: 16, training loss: 0.029032215476036072\n",
      "Epoch: 16, training loss: 0.009615249000489712\n",
      "Epoch: 16, training loss: 0.02131972461938858\n",
      "Epoch: 16, training loss: 0.029137613251805305\n",
      "Epoch: 16, training loss: 0.029563548043370247\n",
      "Epoch: 16, training loss: 0.01962411031126976\n",
      "Epoch: 16, training loss: 0.018443159759044647\n",
      "Epoch: 16, training loss: 0.020864076912403107\n",
      "Epoch: 16, training loss: 0.028648270294070244\n",
      "Epoch: 16, training loss: 0.003909368999302387\n",
      "Epoch: 16, training loss: 0.012318181805312634\n",
      "Epoch: 16, training loss: 0.008417326956987381\n",
      "Epoch: 16, training loss: 0.00546024926006794\n",
      "Epoch: 16, training loss: 0.01635904610157013\n",
      "Epoch: 16, training loss: 0.008766998536884785\n",
      "Epoch: 16, training loss: 0.014388789422810078\n",
      "Epoch: 16, training loss: 0.010429175570607185\n",
      "Epoch: 16, training loss: 0.015925342217087746\n",
      "Epoch: 16, training loss: 0.01656552217900753\n",
      "Epoch: 16, training loss: 0.028417004272341728\n",
      "Epoch: 16, training loss: 0.010344471782445908\n",
      "Epoch: 16, training loss: 0.02788636088371277\n",
      "Epoch: 16, training loss: 0.024308674037456512\n",
      "Epoch: 16, training loss: 0.016192149370908737\n",
      "Epoch: 16, training loss: 0.010359066538512707\n",
      "Epoch: 16, training loss: 0.0063045816496014595\n",
      "Epoch: 16, training loss: 0.019442938268184662\n",
      "Epoch: 16, training loss: 0.03236044570803642\n",
      "Epoch: 16, training loss: 0.00438997708261013\n",
      "Epoch: 16, training loss: 0.02251703105866909\n",
      "Epoch: 16, training loss: 0.010655997321009636\n",
      "Epoch: 16, training loss: 0.015065846033394337\n",
      "Epoch: 16, training loss: 0.048018284142017365\n",
      "Epoch: 16, training loss: 0.0351148322224617\n",
      "Epoch: 16, training loss: 0.010564299300312996\n",
      "Epoch: 16, training loss: 0.02273409254848957\n",
      "Epoch: 16, training loss: 0.031710416078567505\n",
      "Epoch: 16, training loss: 0.025634771212935448\n",
      "Epoch: 16, training loss: 0.023700596764683723\n",
      "Epoch: 16, training loss: 0.009065353311598301\n",
      "Epoch: 16, training loss: 0.030617142096161842\n",
      "Epoch: 16, training loss: 0.03631624951958656\n",
      "Epoch: 16, training loss: 0.02571450173854828\n",
      "Epoch: 16, training loss: 0.03211534395813942\n",
      "Epoch: 16, training loss: 0.03224898502230644\n",
      "Epoch 16 Completed: Avg Loss: 0.0227, Accuracy: 99.42%\n",
      "Epoch: 16, AVG Loss: 0.0006680156545092662, accuracy per epoch: 97.53333333333333\n",
      "Epoch: 17, training loss: 0.03325582295656204\n",
      "Epoch: 17, training loss: 0.012392462231218815\n",
      "Epoch: 17, training loss: 0.024746669456362724\n",
      "Epoch: 17, training loss: 0.007464778609573841\n",
      "Epoch: 17, training loss: 0.014968298375606537\n",
      "Epoch: 17, training loss: 0.018647272139787674\n",
      "Epoch: 17, training loss: 0.02853010594844818\n",
      "Epoch: 17, training loss: 0.01820601522922516\n",
      "Epoch: 17, training loss: 0.01626375876367092\n",
      "Epoch: 17, training loss: 0.010123154148459435\n",
      "Epoch: 17, training loss: 0.014602923765778542\n",
      "Epoch: 17, training loss: 0.02178722620010376\n",
      "Epoch: 17, training loss: 0.022006819024682045\n",
      "Epoch: 17, training loss: 0.044169679284095764\n",
      "Epoch: 17, training loss: 0.01108110137283802\n",
      "Epoch: 17, training loss: 0.024412795901298523\n",
      "Epoch: 17, training loss: 0.024766484275460243\n",
      "Epoch: 17, training loss: 0.010605116374790668\n",
      "Epoch: 17, training loss: 0.03453333303332329\n",
      "Epoch: 17, training loss: 0.008877757005393505\n",
      "Epoch: 17, training loss: 0.019876694306731224\n",
      "Epoch: 17, training loss: 0.011711075901985168\n",
      "Epoch: 17, training loss: 0.013459802605211735\n",
      "Epoch: 17, training loss: 0.014688189141452312\n",
      "Epoch: 17, training loss: 0.013627161271870136\n",
      "Epoch: 17, training loss: 0.014821660704910755\n",
      "Epoch: 17, training loss: 0.019219757989048958\n",
      "Epoch: 17, training loss: 0.014882395975291729\n",
      "Epoch: 17, training loss: 0.026089180260896683\n",
      "Epoch: 17, training loss: 0.021493548527359962\n",
      "Epoch: 17, training loss: 0.02467971108853817\n",
      "Epoch: 17, training loss: 0.03941204398870468\n",
      "Epoch: 17, training loss: 0.015410709194839\n",
      "Epoch: 17, training loss: 0.010488836094737053\n",
      "Epoch: 17, training loss: 0.026273608207702637\n",
      "Epoch: 17, training loss: 0.013321560807526112\n",
      "Epoch: 17, training loss: 0.019504718482494354\n",
      "Epoch: 17, training loss: 0.017712075263261795\n",
      "Epoch: 17, training loss: 0.021868474781513214\n",
      "Epoch: 17, training loss: 0.03995025157928467\n",
      "Epoch: 17, training loss: 0.03543977066874504\n",
      "Epoch: 17, training loss: 0.026271458715200424\n",
      "Epoch: 17, training loss: 0.01652987115085125\n",
      "Epoch: 17, training loss: 0.0047416528686881065\n",
      "Epoch: 17, training loss: 0.009395687840878963\n",
      "Epoch 17 Completed: Avg Loss: 0.0204, Accuracy: 99.52%\n",
      "Epoch: 17, AVG Loss: 0.0006731584860632817, accuracy per epoch: 97.56666666666666\n",
      "Epoch: 18, training loss: 0.01953847147524357\n",
      "Epoch: 18, training loss: 0.006572020705789328\n",
      "Epoch: 18, training loss: 0.011927352286875248\n",
      "Epoch: 18, training loss: 0.021051421761512756\n",
      "Epoch: 18, training loss: 0.009250621311366558\n",
      "Epoch: 18, training loss: 0.010439394973218441\n",
      "Epoch: 18, training loss: 0.00839055422693491\n",
      "Epoch: 18, training loss: 0.01071956753730774\n",
      "Epoch: 18, training loss: 0.010983418673276901\n",
      "Epoch: 18, training loss: 0.005488562863320112\n",
      "Epoch: 18, training loss: 0.0050782980397343636\n",
      "Epoch: 18, training loss: 0.039659950882196426\n",
      "Epoch: 18, training loss: 0.014623780734837055\n",
      "Epoch: 18, training loss: 0.010143859311938286\n",
      "Epoch: 18, training loss: 0.006327625829726458\n",
      "Epoch: 18, training loss: 0.009993069805204868\n",
      "Epoch: 18, training loss: 0.042194806039333344\n",
      "Epoch: 18, training loss: 0.01553971879184246\n",
      "Epoch: 18, training loss: 0.00928411539644003\n",
      "Epoch: 18, training loss: 0.0770106390118599\n",
      "Epoch: 18, training loss: 0.015007062815129757\n",
      "Epoch: 18, training loss: 0.007452772464603186\n",
      "Epoch: 18, training loss: 0.02855660580098629\n",
      "Epoch: 18, training loss: 0.008189031854271889\n",
      "Epoch: 18, training loss: 0.008455315604805946\n",
      "Epoch: 18, training loss: 0.01155876275151968\n",
      "Epoch: 18, training loss: 0.022959411144256592\n",
      "Epoch: 18, training loss: 0.011793125420808792\n",
      "Epoch: 18, training loss: 0.00688738189637661\n",
      "Epoch: 18, training loss: 0.014295147731900215\n",
      "Epoch: 18, training loss: 0.009515255689620972\n",
      "Epoch: 18, training loss: 0.010861738584935665\n",
      "Epoch: 18, training loss: 0.010223464109003544\n",
      "Epoch: 18, training loss: 0.024702038615942\n",
      "Epoch: 18, training loss: 0.025187546387314796\n",
      "Epoch: 18, training loss: 0.02276097796857357\n",
      "Epoch: 18, training loss: 0.006577374413609505\n",
      "Epoch: 18, training loss: 0.009378541260957718\n",
      "Epoch: 18, training loss: 0.05068259313702583\n",
      "Epoch: 18, training loss: 0.02163519524037838\n",
      "Epoch: 18, training loss: 0.008960481733083725\n",
      "Epoch: 18, training loss: 0.021924704313278198\n",
      "Epoch: 18, training loss: 0.015991969034075737\n",
      "Epoch: 18, training loss: 0.06369750201702118\n",
      "Epoch: 18, training loss: 0.004084634128957987\n",
      "Epoch 18 Completed: Avg Loss: 0.0173, Accuracy: 99.67%\n",
      "Epoch: 18, AVG Loss: 0.0006860117372125387, accuracy per epoch: 97.66666666666667\n",
      "Epoch: 19, training loss: 0.013448995538055897\n",
      "Epoch: 19, training loss: 0.009562263265252113\n",
      "Epoch: 19, training loss: 0.006647596601396799\n",
      "Epoch: 19, training loss: 0.024864252656698227\n",
      "Epoch: 19, training loss: 0.0032085119746625423\n",
      "Epoch: 19, training loss: 0.015793204307556152\n",
      "Epoch: 19, training loss: 0.014487269334495068\n",
      "Epoch: 19, training loss: 0.011804400943219662\n",
      "Epoch: 19, training loss: 0.008566186763346195\n",
      "Epoch: 19, training loss: 0.00772266648709774\n",
      "Epoch: 19, training loss: 0.006006332114338875\n",
      "Epoch: 19, training loss: 0.016897400841116905\n",
      "Epoch: 19, training loss: 0.02023818902671337\n",
      "Epoch: 19, training loss: 0.013997928239405155\n",
      "Epoch: 19, training loss: 0.018686285242438316\n",
      "Epoch: 19, training loss: 0.004930406808853149\n",
      "Epoch: 19, training loss: 0.0048385607078671455\n",
      "Epoch: 19, training loss: 0.004695171024650335\n",
      "Epoch: 19, training loss: 0.022768156602978706\n",
      "Epoch: 19, training loss: 0.016017159447073936\n",
      "Epoch: 19, training loss: 0.021764807403087616\n",
      "Epoch: 19, training loss: 0.00864778645336628\n",
      "Epoch: 19, training loss: 0.005037335678935051\n",
      "Epoch: 19, training loss: 0.014640534296631813\n",
      "Epoch: 19, training loss: 0.018813639879226685\n",
      "Epoch: 19, training loss: 0.026989154517650604\n",
      "Epoch: 19, training loss: 0.006448827218264341\n",
      "Epoch: 19, training loss: 0.004903186112642288\n",
      "Epoch: 19, training loss: 0.012070800177752972\n",
      "Epoch: 19, training loss: 0.0034487613011151552\n",
      "Epoch: 19, training loss: 0.024344010278582573\n",
      "Epoch: 19, training loss: 0.017015941441059113\n",
      "Epoch: 19, training loss: 0.013978090137243271\n",
      "Epoch: 19, training loss: 0.006114518735557795\n",
      "Epoch: 19, training loss: 0.013015469536185265\n",
      "Epoch: 19, training loss: 0.0059157470241189\n",
      "Epoch: 19, training loss: 0.009803086519241333\n",
      "Epoch: 19, training loss: 0.004578459542244673\n",
      "Epoch: 19, training loss: 0.02343112975358963\n",
      "Epoch: 19, training loss: 0.004727495834231377\n",
      "Epoch: 19, training loss: 0.013178561814129353\n",
      "Epoch: 19, training loss: 0.036815542727708817\n",
      "Epoch: 19, training loss: 0.05557487532496452\n",
      "Epoch: 19, training loss: 0.008833005093038082\n",
      "Epoch: 19, training loss: 0.011311852373182774\n",
      "Epoch 19 Completed: Avg Loss: 0.0149, Accuracy: 99.73%\n",
      "Epoch: 19, AVG Loss: 0.0006763335472593705, accuracy per epoch: 97.39999999999999\n",
      "Epoch: 20, training loss: 0.01117871142923832\n",
      "Epoch: 20, training loss: 0.01017201878130436\n",
      "Epoch: 20, training loss: 0.008943697437644005\n",
      "Epoch: 20, training loss: 0.018413467332720757\n",
      "Epoch: 20, training loss: 0.015598399564623833\n",
      "Epoch: 20, training loss: 0.007876364514231682\n",
      "Epoch: 20, training loss: 0.008784410543739796\n",
      "Epoch: 20, training loss: 0.013616476207971573\n",
      "Epoch: 20, training loss: 0.005875256843864918\n",
      "Epoch: 20, training loss: 0.01937767118215561\n",
      "Epoch: 20, training loss: 0.012579882517457008\n",
      "Epoch: 20, training loss: 0.01561067346483469\n",
      "Epoch: 20, training loss: 0.008509554900228977\n",
      "Epoch: 20, training loss: 0.009471344761550426\n",
      "Epoch: 20, training loss: 0.016533557325601578\n",
      "Epoch: 20, training loss: 0.005492397118359804\n",
      "Epoch: 20, training loss: 0.021482327952980995\n",
      "Epoch: 20, training loss: 0.007204959634691477\n",
      "Epoch: 20, training loss: 0.010478165931999683\n",
      "Epoch: 20, training loss: 0.009091482497751713\n",
      "Epoch: 20, training loss: 0.01453486829996109\n",
      "Epoch: 20, training loss: 0.013221506960690022\n",
      "Epoch: 20, training loss: 0.02231615036725998\n",
      "Epoch: 20, training loss: 0.009366038255393505\n",
      "Epoch: 20, training loss: 0.018486514687538147\n",
      "Epoch: 20, training loss: 0.008771294727921486\n",
      "Epoch: 20, training loss: 0.032082103192806244\n",
      "Epoch: 20, training loss: 0.009049824438989162\n",
      "Epoch: 20, training loss: 0.010783547535538673\n",
      "Epoch: 20, training loss: 0.013253598473966122\n",
      "Epoch: 20, training loss: 0.00879372376948595\n",
      "Epoch: 20, training loss: 0.036818407475948334\n",
      "Epoch: 20, training loss: 0.0057345228269696236\n",
      "Epoch: 20, training loss: 0.008653626777231693\n",
      "Epoch: 20, training loss: 0.016725704073905945\n",
      "Epoch: 20, training loss: 0.010133340023458004\n",
      "Epoch: 20, training loss: 0.0043975296430289745\n",
      "Epoch: 20, training loss: 0.010280639864504337\n",
      "Epoch: 20, training loss: 0.00974738597869873\n",
      "Epoch: 20, training loss: 0.014073438942432404\n",
      "Epoch: 20, training loss: 0.011070992797613144\n",
      "Epoch: 20, training loss: 0.022126153111457825\n",
      "Epoch: 20, training loss: 0.0036094386596232653\n",
      "Epoch: 20, training loss: 0.021539393812417984\n",
      "Epoch: 20, training loss: 0.008446910418570042\n",
      "Epoch 20 Completed: Avg Loss: 0.0134, Accuracy: 99.78%\n",
      "Epoch: 20, AVG Loss: 0.0006328620392208298, accuracy per epoch: 97.83333333333334\n",
      "Epoch: 21, training loss: 0.006893163081258535\n",
      "Epoch: 21, training loss: 0.007393199019134045\n",
      "Epoch: 21, training loss: 0.004850867670029402\n",
      "Epoch: 21, training loss: 0.05536501482129097\n",
      "Epoch: 21, training loss: 0.012090543285012245\n",
      "Epoch: 21, training loss: 0.0029195640236139297\n",
      "Epoch: 21, training loss: 0.019202858209609985\n",
      "Epoch: 21, training loss: 0.00834215059876442\n",
      "Epoch: 21, training loss: 0.006762067321687937\n",
      "Epoch: 21, training loss: 0.012711034156382084\n",
      "Epoch: 21, training loss: 0.005929502192884684\n",
      "Epoch: 21, training loss: 0.004721083678305149\n",
      "Epoch: 21, training loss: 0.009287314489483833\n",
      "Epoch: 21, training loss: 0.016273673623800278\n",
      "Epoch: 21, training loss: 0.005424871109426022\n",
      "Epoch: 21, training loss: 0.021736670285463333\n",
      "Epoch: 21, training loss: 0.009228464215993881\n",
      "Epoch: 21, training loss: 0.010754449293017387\n",
      "Epoch: 21, training loss: 0.022044658660888672\n",
      "Epoch: 21, training loss: 0.005725329741835594\n",
      "Epoch: 21, training loss: 0.011934410780668259\n",
      "Epoch: 21, training loss: 0.006453583482652903\n",
      "Epoch: 21, training loss: 0.010332384146749973\n",
      "Epoch: 21, training loss: 0.008587892167270184\n",
      "Epoch: 21, training loss: 0.007842116057872772\n",
      "Epoch: 21, training loss: 0.017500244081020355\n",
      "Epoch: 21, training loss: 0.0037904216442257166\n",
      "Epoch: 21, training loss: 0.0023340978659689426\n",
      "Epoch: 21, training loss: 0.004964570049196482\n",
      "Epoch: 21, training loss: 0.013788014650344849\n",
      "Epoch: 21, training loss: 0.007028957363218069\n",
      "Epoch: 21, training loss: 0.008628740906715393\n",
      "Epoch: 21, training loss: 0.0077258991077542305\n",
      "Epoch: 21, training loss: 0.007866143248975277\n",
      "Epoch: 21, training loss: 0.0014569894410669804\n",
      "Epoch: 21, training loss: 0.005375646520406008\n",
      "Epoch: 21, training loss: 0.025343796238303185\n",
      "Epoch: 21, training loss: 0.0027671153657138348\n",
      "Epoch: 21, training loss: 0.004938922356814146\n",
      "Epoch: 21, training loss: 0.020358148962259293\n",
      "Epoch: 21, training loss: 0.012932232581079006\n",
      "Epoch: 21, training loss: 0.00979522056877613\n",
      "Epoch: 21, training loss: 0.003938321024179459\n",
      "Epoch: 21, training loss: 0.0069908094592392445\n",
      "Epoch: 21, training loss: 0.010230577550828457\n",
      "Epoch 21 Completed: Avg Loss: 0.0117, Accuracy: 99.82%\n",
      "Epoch: 21, AVG Loss: 0.0006460370495915413, accuracy per epoch: 97.8\n",
      "Epoch: 22, training loss: 0.00454077310860157\n",
      "Epoch: 22, training loss: 0.017338385805487633\n",
      "Epoch: 22, training loss: 0.01196235604584217\n",
      "Epoch: 22, training loss: 0.0050431909039616585\n",
      "Epoch: 22, training loss: 0.013621935620903969\n",
      "Epoch: 22, training loss: 0.01715805009007454\n",
      "Epoch: 22, training loss: 0.004596562124788761\n",
      "Epoch: 22, training loss: 0.009290680289268494\n",
      "Epoch: 22, training loss: 0.006696606054902077\n",
      "Epoch: 22, training loss: 0.01219630241394043\n",
      "Epoch: 22, training loss: 0.008938371203839779\n",
      "Epoch: 22, training loss: 0.01524191815406084\n",
      "Epoch: 22, training loss: 0.005576447583734989\n",
      "Epoch: 22, training loss: 0.005127833224833012\n",
      "Epoch: 22, training loss: 0.003778825979679823\n",
      "Epoch: 22, training loss: 0.0028913579881191254\n",
      "Epoch: 22, training loss: 0.00897224247455597\n",
      "Epoch: 22, training loss: 0.001821194076910615\n",
      "Epoch: 22, training loss: 0.020172299817204475\n",
      "Epoch: 22, training loss: 0.0026919308584183455\n",
      "Epoch: 22, training loss: 0.006306104827672243\n",
      "Epoch: 22, training loss: 0.006853688973933458\n",
      "Epoch: 22, training loss: 0.019951773807406425\n",
      "Epoch: 22, training loss: 0.006283450406044722\n",
      "Epoch: 22, training loss: 0.003261844627559185\n",
      "Epoch: 22, training loss: 0.0023911285679787397\n",
      "Epoch: 22, training loss: 0.00597668532282114\n",
      "Epoch: 22, training loss: 0.01963150128722191\n",
      "Epoch: 22, training loss: 0.04350724443793297\n",
      "Epoch: 22, training loss: 0.01104239746928215\n",
      "Epoch: 22, training loss: 0.020692553371191025\n",
      "Epoch: 22, training loss: 0.010475046001374722\n",
      "Epoch: 22, training loss: 0.013928753323853016\n",
      "Epoch: 22, training loss: 0.00979591067880392\n",
      "Epoch: 22, training loss: 0.004699076525866985\n",
      "Epoch: 22, training loss: 0.005919221322983503\n",
      "Epoch: 22, training loss: 0.008307447656989098\n",
      "Epoch: 22, training loss: 0.00567924790084362\n",
      "Epoch: 22, training loss: 0.0038852982688695192\n",
      "Epoch: 22, training loss: 0.0052069188095629215\n",
      "Epoch: 22, training loss: 0.005969001445919275\n",
      "Epoch: 22, training loss: 0.012618409469723701\n",
      "Epoch: 22, training loss: 0.008337389677762985\n",
      "Epoch: 22, training loss: 0.0033233866561204195\n",
      "Epoch: 22, training loss: 0.010431420989334583\n",
      "Epoch 22 Completed: Avg Loss: 0.0104, Accuracy: 99.86%\n",
      "Epoch: 22, AVG Loss: 0.0006637340318411589, accuracy per epoch: 97.83333333333334\n",
      "Epoch: 23, training loss: 0.008048088289797306\n",
      "Epoch: 23, training loss: 0.005767493508756161\n",
      "Epoch: 23, training loss: 0.006717581767588854\n",
      "Epoch: 23, training loss: 0.006540344096720219\n",
      "Epoch: 23, training loss: 0.020029494538903236\n",
      "Epoch: 23, training loss: 0.0038647849578410387\n",
      "Epoch: 23, training loss: 0.008601329289376736\n",
      "Epoch: 23, training loss: 0.0063472650945186615\n",
      "Epoch: 23, training loss: 0.003107161493971944\n",
      "Epoch: 23, training loss: 0.011447396129369736\n",
      "Epoch: 23, training loss: 0.010782988741993904\n",
      "Epoch: 23, training loss: 0.011720196343958378\n",
      "Epoch: 23, training loss: 0.005480959080159664\n",
      "Epoch: 23, training loss: 0.011545120738446712\n",
      "Epoch: 23, training loss: 0.0073232101276516914\n",
      "Epoch: 23, training loss: 0.009165461175143719\n",
      "Epoch: 23, training loss: 0.009965338744223118\n",
      "Epoch: 23, training loss: 0.008026571944355965\n",
      "Epoch: 23, training loss: 0.004963644780218601\n",
      "Epoch: 23, training loss: 0.005497779697179794\n",
      "Epoch: 23, training loss: 0.007418948225677013\n",
      "Epoch: 23, training loss: 0.008597681298851967\n",
      "Epoch: 23, training loss: 0.006081005558371544\n",
      "Epoch: 23, training loss: 0.0027880582492798567\n",
      "Epoch: 23, training loss: 0.01758512668311596\n",
      "Epoch: 23, training loss: 0.004590839613229036\n",
      "Epoch: 23, training loss: 0.011282014660537243\n",
      "Epoch: 23, training loss: 0.006939595565199852\n",
      "Epoch: 23, training loss: 0.007914617657661438\n",
      "Epoch: 23, training loss: 0.009798632934689522\n",
      "Epoch: 23, training loss: 0.009064878337085247\n",
      "Epoch: 23, training loss: 0.004940057639032602\n",
      "Epoch: 23, training loss: 0.006515831686556339\n",
      "Epoch: 23, training loss: 0.013786853291094303\n",
      "Epoch: 23, training loss: 0.005442457739263773\n",
      "Epoch: 23, training loss: 0.005297234281897545\n",
      "Epoch: 23, training loss: 0.014422165229916573\n",
      "Epoch: 23, training loss: 0.03543617203831673\n",
      "Epoch: 23, training loss: 0.030965160578489304\n",
      "Epoch: 23, training loss: 0.005154349375516176\n",
      "Epoch: 23, training loss: 0.013191528618335724\n",
      "Epoch: 23, training loss: 0.007131925784051418\n",
      "Epoch: 23, training loss: 0.004812606610357761\n",
      "Epoch: 23, training loss: 0.007126885931938887\n",
      "Epoch: 23, training loss: 0.004884533584117889\n",
      "Epoch 23 Completed: Avg Loss: 0.0091, Accuracy: 99.89%\n",
      "Epoch: 23, AVG Loss: 0.0006639101831242443, accuracy per epoch: 97.7\n",
      "Epoch: 24, training loss: 0.0029522289987653494\n",
      "Epoch: 24, training loss: 0.02136525698006153\n",
      "Epoch: 24, training loss: 0.003236732678487897\n",
      "Epoch: 24, training loss: 0.006258022040128708\n",
      "Epoch: 24, training loss: 0.007693709339946508\n",
      "Epoch: 24, training loss: 0.004563842434436083\n",
      "Epoch: 24, training loss: 0.0067847236059606075\n",
      "Epoch: 24, training loss: 0.0032631061039865017\n",
      "Epoch: 24, training loss: 0.00407699029892683\n",
      "Epoch: 24, training loss: 0.0031069887336343527\n",
      "Epoch: 24, training loss: 0.009489371441304684\n",
      "Epoch: 24, training loss: 0.0027655656449496746\n",
      "Epoch: 24, training loss: 0.01805151253938675\n",
      "Epoch: 24, training loss: 0.006164480000734329\n",
      "Epoch: 24, training loss: 0.007218950893729925\n",
      "Epoch: 24, training loss: 0.00414506159722805\n",
      "Epoch: 24, training loss: 0.010224886238574982\n",
      "Epoch: 24, training loss: 0.00829521007835865\n",
      "Epoch: 24, training loss: 0.01318690087646246\n",
      "Epoch: 24, training loss: 0.0035103533882647753\n",
      "Epoch: 24, training loss: 0.01894126646220684\n",
      "Epoch: 24, training loss: 0.003830307861790061\n",
      "Epoch: 24, training loss: 0.04547300189733505\n",
      "Epoch: 24, training loss: 0.007739044725894928\n",
      "Epoch: 24, training loss: 0.0077515533193945885\n",
      "Epoch: 24, training loss: 0.006250949110835791\n",
      "Epoch: 24, training loss: 0.008324791677296162\n",
      "Epoch: 24, training loss: 0.004358647856861353\n",
      "Epoch: 24, training loss: 0.004483435302972794\n",
      "Epoch: 24, training loss: 0.012122941203415394\n",
      "Epoch: 24, training loss: 0.004217574372887611\n",
      "Epoch: 24, training loss: 0.012989700771868229\n",
      "Epoch: 24, training loss: 0.00719060655683279\n",
      "Epoch: 24, training loss: 0.00897581409662962\n",
      "Epoch: 24, training loss: 0.006849843077361584\n",
      "Epoch: 24, training loss: 0.003277213778346777\n",
      "Epoch: 24, training loss: 0.008481884375214577\n",
      "Epoch: 24, training loss: 0.004737385082989931\n",
      "Epoch: 24, training loss: 0.004786722827702761\n",
      "Epoch: 24, training loss: 0.011672820895910263\n",
      "Epoch: 24, training loss: 0.010348564013838768\n",
      "Epoch: 24, training loss: 0.005869324319064617\n",
      "Epoch: 24, training loss: 0.012735013850033283\n",
      "Epoch: 24, training loss: 0.003532982664182782\n",
      "Epoch: 24, training loss: 0.003054784145206213\n",
      "Epoch 24 Completed: Avg Loss: 0.0081, Accuracy: 99.92%\n",
      "Epoch: 24, AVG Loss: 0.0006737498212605714, accuracy per epoch: 97.73333333333333\n",
      "Epoch: 25, training loss: 0.012063363566994667\n",
      "Epoch: 25, training loss: 0.006080797407776117\n",
      "Epoch: 25, training loss: 0.006773373112082481\n",
      "Epoch: 25, training loss: 0.004127360414713621\n",
      "Epoch: 25, training loss: 0.0020793552976101637\n",
      "Epoch: 25, training loss: 0.004651910159736872\n",
      "Epoch: 25, training loss: 0.0027080033905804157\n",
      "Epoch: 25, training loss: 0.014569857157766819\n",
      "Epoch: 25, training loss: 0.013109628111124039\n",
      "Epoch: 25, training loss: 0.006252089515328407\n",
      "Epoch: 25, training loss: 0.006795129273086786\n",
      "Epoch: 25, training loss: 0.005231399089097977\n",
      "Epoch: 25, training loss: 0.011920086108148098\n",
      "Epoch: 25, training loss: 0.006389317102730274\n",
      "Epoch: 25, training loss: 0.01816697232425213\n",
      "Epoch: 25, training loss: 0.0037298747338354588\n",
      "Epoch: 25, training loss: 0.004921573679894209\n",
      "Epoch: 25, training loss: 0.012045004405081272\n",
      "Epoch: 25, training loss: 0.0028012325055897236\n",
      "Epoch: 25, training loss: 0.004749405663460493\n",
      "Epoch: 25, training loss: 0.01536783017218113\n",
      "Epoch: 25, training loss: 0.007679941598325968\n",
      "Epoch: 25, training loss: 0.007812165655195713\n",
      "Epoch: 25, training loss: 0.005927986931055784\n",
      "Epoch: 25, training loss: 0.0015191857237368822\n",
      "Epoch: 25, training loss: 0.008646831847727299\n",
      "Epoch: 25, training loss: 0.007765674497932196\n",
      "Epoch: 25, training loss: 0.004507537931203842\n",
      "Epoch: 25, training loss: 0.03142654895782471\n",
      "Epoch: 25, training loss: 0.003999172244220972\n",
      "Epoch: 25, training loss: 0.00903667975217104\n",
      "Epoch: 25, training loss: 0.007531922310590744\n",
      "Epoch: 25, training loss: 0.0037096459418535233\n",
      "Epoch: 25, training loss: 0.0057107084430754185\n",
      "Epoch: 25, training loss: 0.01279581617563963\n",
      "Epoch: 25, training loss: 0.01031470950692892\n",
      "Epoch: 25, training loss: 0.006742558442056179\n",
      "Epoch: 25, training loss: 0.007580956909805536\n",
      "Epoch: 25, training loss: 0.008730127476155758\n",
      "Epoch: 25, training loss: 0.0060210805386304855\n",
      "Epoch: 25, training loss: 0.006099184043705463\n",
      "Epoch: 25, training loss: 0.003006226848810911\n",
      "Epoch: 25, training loss: 0.010565540753304958\n",
      "Epoch: 25, training loss: 0.008264921605587006\n",
      "Epoch: 25, training loss: 0.02281995303928852\n",
      "Epoch 25 Completed: Avg Loss: 0.0073, Accuracy: 99.93%\n",
      "Epoch: 25, AVG Loss: 0.0006718526460851232, accuracy per epoch: 97.66666666666667\n",
      "Epoch: 26, training loss: 0.014608653262257576\n",
      "Epoch: 26, training loss: 0.006781151983886957\n",
      "Epoch: 26, training loss: 0.0035730404779314995\n",
      "Epoch: 26, training loss: 0.006015032064169645\n",
      "Epoch: 26, training loss: 0.004767542239278555\n",
      "Epoch: 26, training loss: 0.0067885154858231544\n",
      "Epoch: 26, training loss: 0.0051305959932506084\n",
      "Epoch: 26, training loss: 0.003870988730341196\n",
      "Epoch: 26, training loss: 0.01105746254324913\n",
      "Epoch: 26, training loss: 0.002625136636197567\n",
      "Epoch: 26, training loss: 0.00296246656216681\n",
      "Epoch: 26, training loss: 0.0074902307242155075\n",
      "Epoch: 26, training loss: 0.004326595924794674\n",
      "Epoch: 26, training loss: 0.008200558833777905\n",
      "Epoch: 26, training loss: 0.005730707664042711\n",
      "Epoch: 26, training loss: 0.004759428557008505\n",
      "Epoch: 26, training loss: 0.003141870489344001\n",
      "Epoch: 26, training loss: 0.007639802992343903\n",
      "Epoch: 26, training loss: 0.004762642085552216\n",
      "Epoch: 26, training loss: 0.0087240906432271\n",
      "Epoch: 26, training loss: 0.00504431314766407\n",
      "Epoch: 26, training loss: 0.005962584167718887\n",
      "Epoch: 26, training loss: 0.004560038447380066\n",
      "Epoch: 26, training loss: 0.0029585068114101887\n",
      "Epoch: 26, training loss: 0.00613477174192667\n",
      "Epoch: 26, training loss: 0.005853918846696615\n",
      "Epoch: 26, training loss: 0.003284194739535451\n",
      "Epoch: 26, training loss: 0.006552782841026783\n",
      "Epoch: 26, training loss: 0.009951505810022354\n",
      "Epoch: 26, training loss: 0.0029267799109220505\n",
      "Epoch: 26, training loss: 0.004382121842354536\n",
      "Epoch: 26, training loss: 0.006704505532979965\n",
      "Epoch: 26, training loss: 0.0033797782380133867\n",
      "Epoch: 26, training loss: 0.006644616834819317\n",
      "Epoch: 26, training loss: 0.0019314081873744726\n",
      "Epoch: 26, training loss: 0.004383275285363197\n",
      "Epoch: 26, training loss: 0.006028508767485619\n",
      "Epoch: 26, training loss: 0.002693105023354292\n",
      "Epoch: 26, training loss: 0.004512439947575331\n",
      "Epoch: 26, training loss: 0.005500496830791235\n",
      "Epoch: 26, training loss: 0.002089390065521002\n",
      "Epoch: 26, training loss: 0.003949017729610205\n",
      "Epoch: 26, training loss: 0.003475731238722801\n",
      "Epoch: 26, training loss: 0.004371959716081619\n",
      "Epoch: 26, training loss: 0.005636065267026424\n",
      "Epoch 26 Completed: Avg Loss: 0.0065, Accuracy: 99.95%\n",
      "Epoch: 26, AVG Loss: 0.0006665309276431799, accuracy per epoch: 97.76666666666667\n",
      "Epoch: 27, training loss: 0.0018704249523580074\n",
      "Epoch: 27, training loss: 0.006614627316594124\n",
      "Epoch: 27, training loss: 0.014148922637104988\n",
      "Epoch: 27, training loss: 0.006380509119480848\n",
      "Epoch: 27, training loss: 0.0037598926573991776\n",
      "Epoch: 27, training loss: 0.0035729175433516502\n",
      "Epoch: 27, training loss: 0.004043666645884514\n",
      "Epoch: 27, training loss: 0.009595317766070366\n",
      "Epoch: 27, training loss: 0.00856300350278616\n",
      "Epoch: 27, training loss: 0.007455834187567234\n",
      "Epoch: 27, training loss: 0.0018085420597344637\n",
      "Epoch: 27, training loss: 0.0033876753877848387\n",
      "Epoch: 27, training loss: 0.011564238928258419\n",
      "Epoch: 27, training loss: 0.00834769569337368\n",
      "Epoch: 27, training loss: 0.00918093416839838\n",
      "Epoch: 27, training loss: 0.006668109446763992\n",
      "Epoch: 27, training loss: 0.0036494992673397064\n",
      "Epoch: 27, training loss: 0.011808429844677448\n",
      "Epoch: 27, training loss: 0.009339872747659683\n",
      "Epoch: 27, training loss: 0.002286094008013606\n",
      "Epoch: 27, training loss: 0.0022880532778799534\n",
      "Epoch: 27, training loss: 0.004277689382433891\n",
      "Epoch: 27, training loss: 0.004665301647037268\n",
      "Epoch: 27, training loss: 0.002667442662641406\n",
      "Epoch: 27, training loss: 0.004083920735865831\n",
      "Epoch: 27, training loss: 0.004787573125213385\n",
      "Epoch: 27, training loss: 0.0029778238385915756\n",
      "Epoch: 27, training loss: 0.0041886065155267715\n",
      "Epoch: 27, training loss: 0.0024747683200985193\n",
      "Epoch: 27, training loss: 0.005878659896552563\n",
      "Epoch: 27, training loss: 0.006535021588206291\n",
      "Epoch: 27, training loss: 0.0034833354875445366\n",
      "Epoch: 27, training loss: 0.00445506302639842\n",
      "Epoch: 27, training loss: 0.006840168032795191\n",
      "Epoch: 27, training loss: 0.004785939585417509\n",
      "Epoch: 27, training loss: 0.006957882549613714\n",
      "Epoch: 27, training loss: 0.006147515028715134\n",
      "Epoch: 27, training loss: 0.004380680155009031\n",
      "Epoch: 27, training loss: 0.0018510714871808887\n",
      "Epoch: 27, training loss: 0.012590065598487854\n",
      "Epoch: 27, training loss: 0.005193744786083698\n",
      "Epoch: 27, training loss: 0.008675300516188145\n",
      "Epoch: 27, training loss: 0.009014541283249855\n",
      "Epoch: 27, training loss: 0.002362508326768875\n",
      "Epoch: 27, training loss: 0.004253343213349581\n",
      "Epoch 27 Completed: Avg Loss: 0.0059, Accuracy: 99.95%\n",
      "Epoch: 27, AVG Loss: 0.0006767731501410405, accuracy per epoch: 97.76666666666667\n",
      "Epoch: 28, training loss: 0.0063902465626597404\n",
      "Epoch: 28, training loss: 0.0033524883911013603\n",
      "Epoch: 28, training loss: 0.005055274814367294\n",
      "Epoch: 28, training loss: 0.006195961032062769\n",
      "Epoch: 28, training loss: 0.004141025245189667\n",
      "Epoch: 28, training loss: 0.0023859140928834677\n",
      "Epoch: 28, training loss: 0.0058187213726341724\n",
      "Epoch: 28, training loss: 0.005580300930887461\n",
      "Epoch: 28, training loss: 0.0024231737479567528\n",
      "Epoch: 28, training loss: 0.0043701245449483395\n",
      "Epoch: 28, training loss: 0.004743407014757395\n",
      "Epoch: 28, training loss: 0.004729383159428835\n",
      "Epoch: 28, training loss: 0.003075314685702324\n",
      "Epoch: 28, training loss: 0.0020655456464737654\n",
      "Epoch: 28, training loss: 0.007012058515101671\n",
      "Epoch: 28, training loss: 0.010832429863512516\n",
      "Epoch: 28, training loss: 0.00823571253567934\n",
      "Epoch: 28, training loss: 0.005135431885719299\n",
      "Epoch: 28, training loss: 0.005218254402279854\n",
      "Epoch: 28, training loss: 0.005395399406552315\n",
      "Epoch: 28, training loss: 0.0026918898802250624\n",
      "Epoch: 28, training loss: 0.004187495447695255\n",
      "Epoch: 28, training loss: 0.013751841150224209\n",
      "Epoch: 28, training loss: 0.0026218746788799763\n",
      "Epoch: 28, training loss: 0.006369118113070726\n",
      "Epoch: 28, training loss: 0.005247512366622686\n",
      "Epoch: 28, training loss: 0.004249938298016787\n",
      "Epoch: 28, training loss: 0.02010682225227356\n",
      "Epoch: 28, training loss: 0.0012508472427725792\n",
      "Epoch: 28, training loss: 0.00334715424105525\n",
      "Epoch: 28, training loss: 0.003701187437400222\n",
      "Epoch: 28, training loss: 0.002723370213061571\n",
      "Epoch: 28, training loss: 0.005833595525473356\n",
      "Epoch: 28, training loss: 0.005562973208725452\n",
      "Epoch: 28, training loss: 0.002404996193945408\n",
      "Epoch: 28, training loss: 0.011610111221671104\n",
      "Epoch: 28, training loss: 0.009578367695212364\n",
      "Epoch: 28, training loss: 0.005282312631607056\n",
      "Epoch: 28, training loss: 0.004987763240933418\n",
      "Epoch: 28, training loss: 0.008293199352920055\n",
      "Epoch: 28, training loss: 0.00589732825756073\n",
      "Epoch: 28, training loss: 0.0036983841564506292\n",
      "Epoch: 28, training loss: 0.003467773785814643\n",
      "Epoch: 28, training loss: 0.005646144039928913\n",
      "Epoch: 28, training loss: 0.011567603796720505\n",
      "Epoch 28 Completed: Avg Loss: 0.0052, Accuracy: 99.96%\n",
      "Epoch: 28, AVG Loss: 0.0006954731217895945, accuracy per epoch: 97.63333333333334\n",
      "Epoch: 29, training loss: 0.0018001230200752616\n",
      "Epoch: 29, training loss: 0.004036375787109137\n",
      "Epoch: 29, training loss: 0.003473617834970355\n",
      "Epoch: 29, training loss: 0.005591120105236769\n",
      "Epoch: 29, training loss: 0.006155937910079956\n",
      "Epoch: 29, training loss: 0.0030773556791245937\n",
      "Epoch: 29, training loss: 0.003784812055528164\n",
      "Epoch: 29, training loss: 0.0023743475321680307\n",
      "Epoch: 29, training loss: 0.0036701099015772343\n",
      "Epoch: 29, training loss: 0.0027283597737550735\n",
      "Epoch: 29, training loss: 0.002233604434877634\n",
      "Epoch: 29, training loss: 0.006239397916942835\n",
      "Epoch: 29, training loss: 0.0018732058815658092\n",
      "Epoch: 29, training loss: 0.0022169172298163176\n",
      "Epoch: 29, training loss: 0.005705374293029308\n",
      "Epoch: 29, training loss: 0.0054039377719163895\n",
      "Epoch: 29, training loss: 0.002428139094263315\n",
      "Epoch: 29, training loss: 0.003089738544076681\n",
      "Epoch: 29, training loss: 0.004922088701277971\n",
      "Epoch: 29, training loss: 0.0038054832257330418\n",
      "Epoch: 29, training loss: 0.008947494439780712\n",
      "Epoch: 29, training loss: 0.0033212159760296345\n",
      "Epoch: 29, training loss: 0.005333616863936186\n",
      "Epoch: 29, training loss: 0.012234247289597988\n",
      "Epoch: 29, training loss: 0.001176923979073763\n",
      "Epoch: 29, training loss: 0.0012473157839849591\n",
      "Epoch: 29, training loss: 0.007618988398462534\n",
      "Epoch: 29, training loss: 0.004089546389877796\n",
      "Epoch: 29, training loss: 0.002317744307219982\n",
      "Epoch: 29, training loss: 0.0024545015767216682\n",
      "Epoch: 29, training loss: 0.007283582352101803\n",
      "Epoch: 29, training loss: 0.003665850730612874\n",
      "Epoch: 29, training loss: 0.02029135264456272\n",
      "Epoch: 29, training loss: 0.007937634363770485\n",
      "Epoch: 29, training loss: 0.003724862588569522\n",
      "Epoch: 29, training loss: 0.01046681683510542\n",
      "Epoch: 29, training loss: 0.0026385514065623283\n",
      "Epoch: 29, training loss: 0.004516707267612219\n",
      "Epoch: 29, training loss: 0.004820358473807573\n",
      "Epoch: 29, training loss: 0.003335248213261366\n",
      "Epoch: 29, training loss: 0.0073438119143247604\n",
      "Epoch: 29, training loss: 0.005103423725813627\n",
      "Epoch: 29, training loss: 0.008224289864301682\n",
      "Epoch: 29, training loss: 0.005306641571223736\n",
      "Epoch: 29, training loss: 0.0017219292931258678\n",
      "Epoch 29 Completed: Avg Loss: 0.0047, Accuracy: 99.98%\n",
      "Epoch: 29, AVG Loss: 0.0007043885930130879, accuracy per epoch: 97.66666666666667\n",
      "Epoch: 30, training loss: 0.0022618507500737906\n",
      "Epoch: 30, training loss: 0.0018388390308246017\n",
      "Epoch: 30, training loss: 0.007106478326022625\n",
      "Epoch: 30, training loss: 0.004377929959446192\n",
      "Epoch: 30, training loss: 0.0030950335785746574\n",
      "Epoch: 30, training loss: 0.0030546318739652634\n",
      "Epoch: 30, training loss: 0.002048495225608349\n",
      "Epoch: 30, training loss: 0.005612722132354975\n",
      "Epoch: 30, training loss: 0.0035900366492569447\n",
      "Epoch: 30, training loss: 0.0034051681868731976\n",
      "Epoch: 30, training loss: 0.004649034235626459\n",
      "Epoch: 30, training loss: 0.003971080295741558\n",
      "Epoch: 30, training loss: 0.0019695437513291836\n",
      "Epoch: 30, training loss: 0.006469234824180603\n",
      "Epoch: 30, training loss: 0.0016069089761003852\n",
      "Epoch: 30, training loss: 0.009237071499228477\n",
      "Epoch: 30, training loss: 0.0016396623104810715\n",
      "Epoch: 30, training loss: 0.0024389727041125298\n",
      "Epoch: 30, training loss: 0.003230754751712084\n",
      "Epoch: 30, training loss: 0.004829988349229097\n",
      "Epoch: 30, training loss: 0.0016158046200871468\n",
      "Epoch: 30, training loss: 0.002851821482181549\n",
      "Epoch: 30, training loss: 0.004246713127940893\n",
      "Epoch: 30, training loss: 0.007065741345286369\n",
      "Epoch: 30, training loss: 0.0032914758194237947\n",
      "Epoch: 30, training loss: 0.0011855649063363671\n",
      "Epoch: 30, training loss: 0.008608035743236542\n",
      "Epoch: 30, training loss: 0.0028135892935097218\n",
      "Epoch: 30, training loss: 0.002105704275891185\n",
      "Epoch: 30, training loss: 0.008656042627990246\n",
      "Epoch: 30, training loss: 0.002360669197514653\n",
      "Epoch: 30, training loss: 0.002756022848188877\n",
      "Epoch: 30, training loss: 0.0043383194133639336\n",
      "Epoch: 30, training loss: 0.006702536717057228\n",
      "Epoch: 30, training loss: 0.004506643395870924\n",
      "Epoch: 30, training loss: 0.006430521607398987\n",
      "Epoch: 30, training loss: 0.00449818279594183\n",
      "Epoch: 30, training loss: 0.002804109128192067\n",
      "Epoch: 30, training loss: 0.005088466685265303\n",
      "Epoch: 30, training loss: 0.0018009040504693985\n",
      "Epoch: 30, training loss: 0.00608372688293457\n",
      "Epoch: 30, training loss: 0.005548175889998674\n",
      "Epoch: 30, training loss: 0.0031156374607235193\n",
      "Epoch: 30, training loss: 0.005644876044243574\n",
      "Epoch: 30, training loss: 0.0073143658228218555\n",
      "Epoch 30 Completed: Avg Loss: 0.0044, Accuracy: 99.97%\n",
      "Epoch: 30, AVG Loss: 0.0006890669610972206, accuracy per epoch: 97.63333333333334\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = funcs.train(model=model,dataset=train_dataloader,hparams=hparam,epoch=epoch,device=device)\n",
    "    save_path = checkpoints_path +  'Teacher_final.tar'\n",
    "    torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'loss': train_loss}, save_path)\n",
    "    validation_loss,validation_acc = funcs.evaluate_model(model=model,dataset=val_loader,epoch=epoch,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, AVG Loss: 0.0004937030494009377, accuracy per epoch: 98.19\n",
      "test accuracy:  98.19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_ , test_accuracy = funcs.evaluate_model(model=model,dataset=test_loader,device=device)\n",
    "print('test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
