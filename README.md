# ğŸ“š Distilling the Knowledge in a Neural Network  

ğŸ” **Paper Re-Implementation** | ğŸ¯ **Knowledge Distillation for Efficient Models**  

This project implements and explores **Knowledge Distillation**, based on the seminal paper  
[*"Distilling the Knowledge in a Neural Network"*](https://arxiv.org/pdf/1503.02531) by **Geoffrey Hinton, Oriol Vinyals, and Jeff Dean**.  

The goal is to train a **smaller, more efficient Student model** that mimics the behavior of a **larger, more complex Teacher model**, preserving performance while reducing computational cost.  

---

## ğŸ“‚ Project Structure  
ğŸ“‚ funcs.py â€“ Core helper functions for training and evaluation.
ğŸ“‚ networks.py â€“ Implementation of the Teacher and Student models.
ğŸ“’ teacher.ipynb â€“ Jupyter Notebook for training the Teacher Model.
ğŸ“’ student.ipynb â€“ Jupyter Notebook for training the Student Model using Knowledge Distillation.


---

## ğŸ’¡ How Knowledge Distillation Works  

ğŸ“Œ **Knowledge Distillation** is a technique where a **large Teacher model** trains a **smaller Student model** by transferring knowledge. The student learns from:  
âœ” **True labels** (hard targets)  
âœ” **Soft probabilities** (soft targets) produced by the Teacher  

### ğŸ“ Key Parameters  
- **Temperature (T)** â€“ Controls the softness of the Teacherâ€™s probability distribution.  
- **Alpha (Î±)** â€“ Balances the contribution of **distillation loss** and **true label loss**.  

---

## ğŸ“Š Results & Key Observations  

âœ” **Higher temperature (T) values led to smoother probability distributions**, improving knowledge transfer.  
âœ” **The Student modelâ€™s accuracy improved post-distillation**, demonstrating the effectiveness of distillation.  
âœ” **A well-trained Student model required significantly fewer resources than the Teacher while maintaining strong performance.**  

ğŸ“‰ **Training Graphs & Accuracy Comparisons** *(Add visual results here if possible!)*  

---

## ğŸš€ Getting Started  

### 1ï¸âƒ£ Install Dependencies  
```bash
pip install torch torchvision numpy matplotlib
